---
title: 6.824 lab3 FTkvserver
author: Leager
mathjax: true
date: 2022-10-08 14:24:54
summary:
categories:
    - 6.824
tags:
    - lab
img:
---

本实验要求在每一个 raft 节点上实现一个 K/V 服务器(server)，向上接受客户端(client)的请求并返回请求结果，向下生成日志应用到 raft 节点中。

整个模式大概长[这样](https://pdos.csail.mit.edu/6.824/notes/raft_diagram.pdf)。

主要流程是这样的：

1. client 寻找对应 raft 节点是 leader 的 server，并发起一个请求(Put/Append/Get)；
2. server 收到请求后，调用 raft 的 Start() 函数；
3. raft 节点间互相 AppendEntries 之后，将消息 apply 到 applyChannel 里；
4. server 从 applyChannel 中取出消息，之后正式将命令应用到数据库中。

<!--more-->

## PART A-Key/value service without snapshots

### 发送请求

由于 Start() 函数会立即返回 isleader 信息，所以如果一个 server 节点调用了 Start() 后发现不是 leader，则返回一个 **ErrWrongLeader**

当且仅当 client RPC 成功并且发送给正确的 leader 后，才算成功；否则：

1. If the Clerk sends an RPC to the wrong kvserver, or if it cannot reach the kvserver, the Clerk should re-try by sending to a different kvserver.
2. If the operation failed to commit (for example, if the leader was replaced), the server reports an error, and the Clerk retries with a different server. 

### server 向下调用

首先是根据 Start() 返回的 isleader 来确定是否需要返回 ErrWrongLeader 错误。若成功，则等待流程 4。

注意到，在等待的过程中 server 是上了锁的，那必然被阻塞，此时就需要在 startKVserver 的时候额外开启一个叫 Applier() 的 goroutine 来进行 applyChannel 的读取。

```go
func (kv *KVServer) Applier() {
    for !kv.killed() {
        msg := <-kv.applyCh
        if msg.CommandValid {
            // 应用操作
        } else if msg.SnapshotValid {
            // 应用快照
        }
    }
}
```

同时遇到这样一个问题： 有一个网络分区   [1] | [2，3，4，5]，1 是分区前的 leader，分区后 1 仍然是 leader。第二个分区由于选主导致其 term 大于 1 的 term。 有一个 client A 一直给 [1] 发送请求，某一个请求 （requestID：x，clientID：A）发过去调用 1 的 start 并返回日志所在索引 index 后，网络恢复，[1] 收到新的 leader 消息后执行 appendEntries 并将原先 index 处的日志覆盖，更新 commitindex 后将覆盖后的日志传回 applyCh，而我采用的是 map[commandIndex] chan 的形式，由于同一索引处的日志被覆盖了，导致 server Applier 处理管道发回的错误请求，比如 get 到错误的值 or 未进行 append

解决方法为：在 arg 中开一个 channel，当 applier 处理完消息时通过该 channel 返回处理结果，同时判断当前 msg 中的 term 和 kv.rf.currentTerm 是否匹配，如果不匹配，说明可能被其他日志覆盖，返回一个 ErrWrongLeader 让上层重发。

> 发现 6.824 labrpc 的 call 函数在传的时候会把 chan 给弄没掉。于是只能在 server 里 args.ch = make(chan int)，同时为了防止遇到的 chan 是 nil，还要加个判断当前是否为 leader，因为只有 leader 的 chan 是有效的，用 call 发给 follower 后会 arg 里的 chan 会变成 nil

### 应用到状态机

Applier() 对 applyChannel 进行一个 for 的等待，收到消息后，根据收到消息的命令类型的不同，执行不同的应用操作。

> Put(key, value) replaces the value for a particular key in the database, Append(key, arg) appends arg to key's value, and Get(key) fetches the current value for the key. A Get for a non-existent key should return an empty string. An Append to a non-existent key should act like Put.

执行完后，返回消息到 waitchannel[index]，唤醒 server。

### Other

1. 刚开始想到和 Raft 里发送请求投票一样的手段，为每一个 server 开一个 goroutine 去发送 RPC，后来发现最后成功发送的只有一个，其它 goroutine 都是在占 cpu，遂放弃，直接用 for 循环。另外，有时候 leader 会在相当一段时间内保持不变，我们可以保存上一次发送请求成功时的 serverId，认为这是 leader，每次发请求时都可以利用这一信息，避免了不必要的 RPC，加快速度。当 RPC 失败，或发生了 ErrWrongLeader，只需要简单的令 leader 切换到下一个即可（0->1->...->n-1->0->...）

    ```go
    func (ck *Clerk) SendRequest(args *Args) string {
        ck.mu.Lock()
        defer ck.mu.Unlock()
        args.RequestId, args.ClerkId = ck.RequestId, ck.ClerkId
    
        for {
            reply := &Reply{}
            ok := ck.servers[ck.volatileLeader].Call("KVServer.HandleRequest", args, reply)
            if ok && reply.Err != ErrWrongLeader {
                ck.RequestId++
                return reply.Value
            }
            ck.volatileLeader = (ck.volatileLeader + 1) % len(ck.servers)
        }
    }
    ```
2. 在分区测试中，有可能发生 leader 超时未 apply 的情况（即一个 leader 被分到了 minority 的网络区），此时需要在 server 等待环节加一个 <-time.After() 的信号接受判断，若超时，则直接返回，并且认为超时也是一种"WrongLeader"。
3. lab3A 还要求我们不能执行同一个 client 的重复请求，那么需要在每个 server 上放一个 clientID 到其最近一次命令序号 requestID 的映射，如果 Worker 收到的这个命令序号已经被执行过了，那么就不再执行，直接返回 ErrDuplicated。Get 是否重复执行无所谓，因为它并不会对数据库产生实质性的影响，主要是防止多次 Put/Append 同一个值。

总体代码量比 raft 少太多，但因为论文中并没有给出很详细的指示，就走了很多弯路，以至于绝大多数时间都在 debug...

## PART B-Key/value service with snapshots

本实验要求在 3A 的基础上加上 snapshot 功能。

> 虽然标的是hard，但代码量更少了

server 不断检测 raftStateSize，如果过大，即当 persist.RaftStateSize() >= kv.maxRaftState 时，将当前 db 状态保存下来，调用 raft 层的 snapshot() 并将 db 状态传入。

要实现有两个函数：

1. **MakeSnapshot()** ：当 raftStateSize 过大时保存 db 状态。
2. **ApplySnapshot()** ：raft 层将 snapshotValid 发到 applyCh，被 server 接收到后执行的操作。

> persist 里的 raftstate 和 index 没有直接关系，所以不能用在 snapshot 里的 index 参数。
